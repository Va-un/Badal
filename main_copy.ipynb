{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nU-zUOWXpcoH"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "pET6WzRjyibb"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import time\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm, trange\n",
    "import utils\n",
    "import models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "ZgyjcaA0yibc"
   },
   "outputs": [],
   "source": [
    "cube_len = 32\n",
    "epoch_count = 400\n",
    "batch_size = 128\n",
    "noise_size = 200\n",
    "d_lr = 0.00005 # discriminator learning rate\n",
    "g_lr = 0.0025 # generator learning rate\n",
    "log_folder = \"logs/\"\n",
    "\n",
    "condition_count = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Hs95YVbLyibc"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "all_models1 = utils.load_all(\"data/chair_all/chair_all/\", contains = '_1.mat') # names ends with a rotation number for 12 rotations, 30 degrees each\n",
    "all_models7 = utils.load_all(\"data/chair_all/chair_all/\", contains = '_7.mat') # 1 and 7 are 0 and 180 degrees respectively\n",
    "\n",
    "train_set1 = torch.from_numpy(all_models1).float()\n",
    "train_set7 = torch.from_numpy(all_models7).float()\n",
    "\n",
    "train_set_all = TensorDataset(train_set1, train_set7)\n",
    "train_loader = DataLoader(dataset=train_set_all, batch_size=batch_size, shuffle=True, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "3jNfd_jOyibd"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")\n",
    "\n",
    "generator = models.Generator(noise_size=(noise_size + 1), cube_resolution=cube_len) # noise size +1 condition value\n",
    "discriminator = models.Discriminator(cube_resolution=cube_len)\n",
    "\n",
    "generator = generator.to(device)\n",
    "discriminator = discriminator.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "C0r4aQ0cyibd"
   },
   "outputs": [],
   "source": [
    "optimizerD = optim.Adam(discriminator.parameters(), lr=d_lr, betas=(0.5, 0.999))\n",
    "optimizerG = optim.Adam(generator.parameters(), lr=g_lr, betas=(0.5, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "KHvK5JnVyibd"
   },
   "outputs": [],
   "source": [
    "\n",
    "class CustomHybridLoss(torch.nn.Module):\n",
    "    def __init__(self, bce_weight=0.5, MSELoss=0.5):\n",
    "        super(CustomHybridLoss, self).__init__()\n",
    "        self.bce_weight = bce_weight\n",
    "        self.MSELoss = MSELoss\n",
    "        self.bce_loss = torch.nn.BCELoss()\n",
    "        self.MSELoss = torch.nn.MSELoss()\n",
    "\n",
    "    def forward(self, predictions, targets, ):\n",
    "        bce = self.bce_loss(predictions, targets)\n",
    "        l1 = self.MSELoss(predictions, targets)\n",
    "        return self.bce_weight * bce + self.MSELoss * l1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "sCqYdIXZyibe"
   },
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "criterion_GAN = CustomHybridLoss(bce_weight=0.8, MSELoss=0.2)\n",
    "def get_gan_loss(tensor,ones):\n",
    "    if(ones):\n",
    "        return criterion_GAN(tensor,Variable(torch.ones_like(tensor.data).to(device), requires_grad=False))\n",
    "    else:\n",
    "        return criterion_GAN(tensor,Variable(torch.zeros_like(tensor.data).to(device), requires_grad=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "9gYnjbTZyibe"
   },
   "outputs": [],
   "source": [
    "def get_noise(b_size = batch_size):\n",
    "    return torch.randn([b_size,noise_size], device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "sI40hia6yibe"
   },
   "outputs": [],
   "source": [
    "def train_GAN_epoch():\n",
    "\n",
    "    g_loss = []\n",
    "    d_loss = []\n",
    "    gen_out = []\n",
    "    train_disc = True\n",
    "\n",
    "    for i, data_c in enumerate(train_loader):\n",
    "\n",
    "        acc_list = []\n",
    "\n",
    "        for c in range(condition_count): # train GAN for each condition\n",
    "\n",
    "            data = data_c[c].to(device)\n",
    "\n",
    "            discriminator.zero_grad()\n",
    "            Dr_output = discriminator(data, c)\n",
    "            errD_real = get_gan_loss(Dr_output,True)\n",
    "\n",
    "            fake = generator(get_noise(data.shape[0]), c)\n",
    "            Df_output = discriminator(fake.detach(), c)\n",
    "            errD_fake = get_gan_loss(Df_output,False)\n",
    "\n",
    "            errD = errD_real + errD_fake\n",
    "\n",
    "            acc_r = Dr_output.mean().item()\n",
    "            acc_f = 1.0 - Df_output.mean().item()\n",
    "            acc = (acc_r + acc_f) / 2.0\n",
    "\n",
    "            acc_list.append(acc) # calculate discriminator accuracy\n",
    "\n",
    "            if (train_disc): # train discriminator if the last batch accuracy is less than 0.95\n",
    "                errD.backward()\n",
    "                optimizerD.step()\n",
    "\n",
    "            generator.zero_grad() # train generator\n",
    "            fake = generator(get_noise(), c)\n",
    "            DGF_output = discriminator(fake, c)\n",
    "            errG = get_gan_loss(DGF_output,True)\n",
    "            errG.backward()\n",
    "            optimizerG.step()\n",
    "\n",
    "            d_loss.append(errD.mean().item())\n",
    "            g_loss.append(errG.mean().item())\n",
    "\n",
    "        generator.zero_grad() # train generator for combined loss\n",
    "        discriminator.zero_grad()\n",
    "\n",
    "        fix_noise = get_noise()\n",
    "\n",
    "        fake0 = generator(fix_noise, 0) # generate for condition 0 and 1\n",
    "        fake1 = generator(fix_noise, 1)\n",
    "\n",
    "        fake1_rot = torch.rot90(fake1, 2, [1, 2]) # rotate condition 1\n",
    "        fake_combined = (fake0 + fake1_rot) / 2.0 # combine them by averaging\n",
    "\n",
    "        DGF_output_c = discriminator(fake_combined, 0) # train generator for combined output\n",
    "        errG_c = get_gan_loss(DGF_output_c,True)\n",
    "        errG_c.backward()\n",
    "        optimizerG.step()\n",
    "\n",
    "        train_disc = np.mean(acc_list) < 0.95 # decide for the next batch\n",
    "\n",
    "    gen_out.append( fake0.detach().cpu() ) # return generated samples for condition 0, 1 and combined\n",
    "    gen_out.append( fake1.detach().cpu() )\n",
    "    gen_out.append( fake_combined.detach().cpu() )\n",
    "\n",
    "    return np.mean(d_loss), np.mean(g_loss) , gen_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "3LGlfMZ7yibf"
   },
   "outputs": [],
   "source": [
    "utils.clear_folder(log_folder) # create log folder\n",
    "log_file = open(log_folder +\"logs.txt\" ,\"a\") # open log file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MV32pVWgyibf",
    "outputId": "d9569b40-7b52-4e74-c3b4-ba28b20a7073"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 400 --> d_loss:0.278 g_loss:2.431 time:18.789: 100%|██████████| 401/401 [2:42:23<00:00, 24.30s/it]  \n"
     ]
    }
   ],
   "source": [
    "d_list = []\n",
    "g_list = []\n",
    "\n",
    "pbar = tqdm( range(epoch_count+1) )\n",
    "for i in pbar :\n",
    "\n",
    "    startTime = time.time()\n",
    "\n",
    "    d_loss, g_loss, gen = train_GAN_epoch() #train GAN for 1 epoch\n",
    "\n",
    "    d_list.append(d_loss) # get discriminator and generator loss\n",
    "    g_list.append(g_loss)\n",
    "\n",
    "    utils.plot_graph([d_list,g_list], log_folder + \"loss_graph\") # plot loss graph up to that epoch\n",
    "\n",
    "    epoch_time = time.time() - startTime\n",
    "\n",
    "    writeString = \"epoch %d --> d_loss:%0.3f g_loss:%0.3f time:%0.3f\" % (i, d_loss, g_loss, epoch_time) # generate log string\n",
    "\n",
    "    pbar.set_description(writeString)\n",
    "    log_file.write(writeString + \"\\n\") # write to log file\n",
    "    log_file.flush()\n",
    "\n",
    "    if(i%10 == 0): # save generated samples for each 10th epoch because it takes a long time to visualize the samples\n",
    "        utils.visualize_all(gen, save=True, name = log_folder + \"samples_epoch\" + str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 497
    },
    "id": "9a8mq0J8zbJ0",
    "outputId": "8ea6b76c-858f-49e6-8b4d-187712750637"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'D:\\\\content\\\\logs\\\\loss_graph.png'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPIL\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Open the image\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m img \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/content/logs/loss_graph.png\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Display the image\u001b[39;00m\n\u001b[0;32m      7\u001b[0m img\n",
      "File \u001b[1;32md:\\programs\\Anaconda\\envs\\Shikai\\Lib\\site-packages\\PIL\\Image.py:3277\u001b[0m, in \u001b[0;36mopen\u001b[1;34m(fp, mode, formats)\u001b[0m\n\u001b[0;32m   3274\u001b[0m     filename \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mrealpath(os\u001b[38;5;241m.\u001b[39mfspath(fp))\n\u001b[0;32m   3276\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m filename:\n\u001b[1;32m-> 3277\u001b[0m     fp \u001b[38;5;241m=\u001b[39m \u001b[43mbuiltins\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3278\u001b[0m     exclusive_fp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   3280\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'D:\\\\content\\\\logs\\\\loss_graph.png'"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "\n",
    "# Open the image\n",
    "img = Image.open('/content/logs/loss_graph.png')\n",
    "\n",
    "# Display the image\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gSUfQzF2zbBC"
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "# Open the image\n",
    "img = Image.open('/content/logs/samples_epoch400.png')\n",
    "\n",
    "# Display the image\n",
    "img.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "85h15JpGz0wo",
    "outputId": "01cfe33e-d132-4b33-cd2b-4f8659eb3047",
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_path = '/content/logs/logs.txt'\n",
    "\n",
    "try:\n",
    "    with open(file_path, 'r') as file:\n",
    "        file_lines = file.readlines()\n",
    "        print(\"File Content:\")\n",
    "        for line in file_lines:\n",
    "            print(line.strip())\n",
    "except FileNotFoundError:\n",
    "    print(f\"File '{file_path}' not found.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Shikai",
   "language": "python",
   "name": "shikai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
